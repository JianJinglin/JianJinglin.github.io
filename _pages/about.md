---
permalink: /
title: "Welcome!"
excerpt: "Welcome!"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---
**Email:** jjian [at] scripps [dot] edu

Hi there! I'm Jinglin Jian (ÁÆÄÈùñÁê≥), a PhD student at [Scripps Research](https://www.scripps.edu/) by the beautiful ocean üèñÔ∏è at San Diego, CA. I'm currently rotating in the [Forli Lab](https://forlilab.org/), where I explore computational approaches for drug discovery. I'm deeply grateful to be supported by the **Kellogg Fellowship**, a three-year endowed award generously funded by the Kellogg family and The ALSAM Foundation.
I received my master degree from the [School of Information Sciences](https://ischool.illinois.edu/) at the [University of Illinois at Urbana-Champaign](https://illinois.edu/), where I had the opportunity to work close with Professor [Qingyun Wang](https://eaglew.github.io/), Professor [Haohan Wang](https://haohanwang.github.io/), and Professor [Ge Liu](https://www.mit.edu/~geliu/). Previously, I studied at [Beijing Normal University](https://english.bnu.edu.cn/), with a dual B.Econ. degree in **Economics** from [Peking University](https://english.pku.edu.cn/).

<style>
table, td, th, tr {
   border: none!important;
   font-size: 14px;
}
</style>

<h2><span>Publications and Conferences</span></h2>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">BPS 2026</div><img src='/images/2026bps-ppk2a.png' alt="2026bps" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
<h3>Allosterically Inhibiting Pseudomonas aeruginosa's Polyphosphate Kinase 2A by Disrupting Its Oligomerization</h3>
<span style="font-family: Georgia, serif; font-size: smaller;">Constanza Torres-Paris, Madeline G. Ammend, Joseph Agha, <b>Jinglin Jian</b>, Matthew Holcomb, Stefano Forli, Lisa R. Racki</span>
<br>
<!-- <a href='#'><button class="paper-btn">Paper</button></a> -->
<br>
<em style="font-family: 'Times New Roman', Times, serif; font-size: smaller;">Performed virtual screening of 2.4 million compounds against Ppk2A mutant proteins using AutoDock-GPU and identified preliminary hit candidates.</em>
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">In Preparation</div><img src='/images/jian2026npj-1.png' alt="jian2026npj" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
<h3>The AI Scientist in Health: Potential, Challenges, and the Road Ahead</h3>
<span style="font-family: Georgia, serif; font-size: smaller;">Advised by Prof. Qingyun Wang (William & Mary) and Prof. Qingyu Chen (Yale)</span>
<br>
<!-- <a href='#'><button class="paper-btn">Paper</button></a>  -->
<br>
<em style="font-family: 'Times New Roman', Times, serif; font-size: smaller;">A paper discussing the potential, challenges, and future direction of AI Scientists in health systems.</em>
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">TechRxiv 2025</div><img src='/images/jian2025survey.png' alt="jian2025survey" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
<h3>Exploring Agentic Multimodal Large Language Models: A Survey for AIScientists</h3>
<span style="font-family: Georgia, serif; font-size: smaller;"> <b>Jinglin Jian</b>, Yi R. Fung, Denghui Zhang, Yiqian Liang, Qingyu Chen, Zhiyong Lu, Qingyun Wang</span>
<br>
<a href='https://doi.org/10.36227/techrxiv.176344216.60619335/v1'><button class="paper-btn">Paper</button></a>
<!-- <a href='https://github.com/EmpathYang/Prejudice-Caprice-Framework/'><button class="code-btn">CODE</button></a> -->
<br>
<em style="font-family: 'Times New Roman', Times, serif; font-size: smaller;">A survey framing AIScientists Development from a Multimodal Large Language Models (MLLM)-based AI agents perspective.</em>
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ACL 2026 Submission</div><img src='/images/wang2026co-discovery.png' alt="ACL2026tutorial" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
<h3>Tutorial: Human-AI Co-Discovery</h3>
<span style="font-family: Georgia, serif; font-size: smaller;"> Qingyun Wang, Wenpeng Yin, Lifu Huang, Yi R. (May) Fung, <b>Jinglin Jian</b>, Xuehang Guo, Ruochen Li</span>
<br>
<!-- <a href='https://openreview.net/pdf?id=B1Ok8gBHfh'><button class="paper-btn">Paper</button></a>  -->
<!-- <a href='https://github.com/EmpathYang/Prejudice-Caprice-Framework/'><button class="code-btn">CODE</button></a> -->
<br>
<em style="font-family: 'Times New Roman', Times, serif; font-size: smaller;">This tutorial is currently under submission to ACL 2026.</em>
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">UIUC ML for Bioinformatics Workshop</div><img src='/images/2024geocm.png' alt="2024geocm" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
<h3>GeoCM: Exploring Consistency Models and EGNNs for Molecular 3D Structure Prediction</h3>
<span style="font-family: Georgia, serif; font-size: smaller;"> Ruibo Hou, Dian Zhou, <b>Jinglin Jian</b>, Ge Liu</span>
<br>
<a href='https://openreview.net/pdf?id=B1Ok8gBHfh'><button class="paper-btn">Paper</button></a> 
<!-- <a href='https://github.com/EmpathYang/Prejudice-Caprice-Framework/'><button class="code-btn">CODE</button></a> -->
<br>
<em style="font-family: 'Times New Roman', Times, serif; font-size: smaller;">Developed a self-supervised model based on the Equivariant Graph Neural Networks (EGNN) and Consistency Models (CM) to predict molecular 3D structures.</em>
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">BigData 2024</div><img src='/images/2024multimodal.png' alt="2024multimodal" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
<h3>Patient Outcome Predictions via A Multimodal Lan- guage Model for Electronic Health Records</h3>
<span style="font-family: Georgia, serif; font-size: smaller;">Zihan Li, <b>Jinglin Jian</b>, Chundian Li, Jinxia Yao, Jin Chen, Yang Zhang</span>
<br>
<a href='https://drive.google.com/file/d/1qkch3s35zjKdzGY71EI-cBFfMcDRlKGc/view?usp=sharing'><button class="paper-btn">Paper</button></a> 
<!-- <a href='https://github.com/EmpathYang/Prejudice-Caprice-Framework/'><button class="code-btn">CODE</button></a> -->
<br>
<em style="font-family: 'Times New Roman', Times, serif; font-size: smaller;">Early prediction of mortality risk and hospital length of stay is critical. We propose a multimodal framework that integrates full-text clinical note embeddings and time-stamped physiological data to jointly model patient outcomes.</em>
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">BigData 2024</div><img src='/images/2024aptamer.png' alt="2024aptamer" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
<h3>Big Data-Driven Computational Aptamer Design Framework via Parallel Monte Carlo Tree Search</h3>
<span style="font-family: Georgia, serif; font-size: smaller;"><b>Jinglin Jian</b>, Zhiheng Jiao, Zihan Li, Jin Chen</span>
<br>
<a href='10.1109/BigData62323.2024.10825454'><button class="paper-btn">Paper</button></a> 
<!-- <a href='https://github.com/EmpathYang/Prejudice-Caprice-Framework/'><button class="code-btn">CODE</button></a> -->
<br>
<em style="font-family: 'Times New Roman', Times, serif; font-size: smaller;">Developed an enhanced parallel Monte Carlo Tree Search framework for designing aptamers with high-affinity and specificity for target proteins.</em>
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">arXiv 2024</div><img src='/images/TAIS-framework.png' alt="2024tais" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
<h3>Toward a Team of AI-made Scientists for Scientific Discovery from Gene Expression Data</h3>
<span style="font-family: Georgia, serif; font-size: smaller;">Haoyang Liu, Yijiang Li, <b>Jinglin Jian</b>, Yuxuan Cheng, Jianrong Lu, Shuyi Guo, Jinglei Zhu, Mianchen Zhang, Miantong Zhang, Haohan Wang</span>
<br>
<a href='https://arxiv.org/abs/2402.12391'><button class="paper-btn">Paper</button></a>
<!-- <a href='https://github.com/EmpathYang/Prejudice-Caprice-Framework/'><button class="code-btn">CODE</button></a> -->
<br>
<em style="font-family: 'Times New Roman', Times, serif; font-size: smaller;">ML can discover disease-predictive genes from gene expression data. We introduced the Team of AI-made Scientists (TAIS), a LLM-based framework for automatic streamlining ML analysis. TAIS consists of simulated roles, including a project manager, data engineer, and domain expert.</em>
</div>
</div>
<!--
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">AIED 2024</div><img src='/images/2024visual-aid.jpg' alt="2024visual-aid" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
<h3>"Which Animal Would You Like to See on Your Flashcards?" Designing Visual Aids Together with Kids Using GIMs</h3>
<span style="font-family: Georgia, serif; font-size: smaller;"> Yiqi Xiao and <b>Jinglin Jian</b></span>
<br>
<a href='https://visual-aid-2165b82d929e.herokuapp.com/'><button class="paper-btn">Website</button></a> 
<br>
<em style="font-family: 'Times New Roman', Times, serif; font-size: smaller;">
Visual aids enhance children's learning. Educational theories emphasize student agency. We developed a platform based on image generative models, specifically tailored for children with autism.
</em>
</div>
</div>
 -->

<h2><span>Selected Projects</span></h2>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Research Assistant</div><img src='/images/2020hypervideo.png' alt="2020hypervideo" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
<h3>The Impact of Productive Failure on Learning Performance and Cognitive Load: Using Hypervideo to Facilitate Online Interactions</h3>
<span style="font-family: Georgia, serif; font-size: smaller;">Xiaojie Niu, Jingjing Zhang, Kate M. Xu, Xuan Wang</span>
<br>
<a href='https://ieeexplore.ieee.org/document/9499919'><button class="paper-btn">Paper</button></a> 
<br>
<em style="font-family: 'Times New Roman', Times, serif; font-size: smaller;">Productive failure is an instructional approach that uses students' cognitive conflicts to enhance their learning. This experimental study investigated the effect of productive failure in a hypervideo environment.</em>
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Bachelor‚Äôs Thesis</div><img src='/images/2021eduKG.png' alt="2021eduKG" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
<h3>Semi-automatic Knowledge Graph Construction</h3>
<span style="font-family: Georgia, serif; font-size: smaller;">Jinglin Jian (Advisor: Prof. Qinhua Zheng)</span>
<br>
<a href='https://JianJinglin.github.io/files/2021eduKG.pdf'><button class="paper-btn">Paper (in Chinese)</button></a>
<br>
<em style="font-family: 'Times New Roman', Times, serif; font-size: smaller;">An interactive system was designed and implemented, enabling domain experts to collaborate with AI models to create educational knowledge graphs (KG) from unstructured text (i.e. lecture transcripts).</em>
</div>
</div>

<!-- 
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICLR Workshop 2024</div><img src='/images/Wizard-agent.png' alt="yang2024llm" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
<h3>If LLM Is the Wizard, Then Code Is the Wand: A Survey on How Code Empowers Large Language Models to Serve as Intelligent Agents</h3>
<span style="font-family: Georgia, serif; font-size: smaller;"><b>Ke Yang</b>\*, Jiateng Liu\*, John Wu, Chaoqi Yang, Yi R. Fung, Sha Li, Zixuan Huang, Xu Cao, Xingyao Wang, Yiquan Wang, Heng Ji, Chengxiang Zhai  (* indicates equal contributions)</span>
<br>
<a href='https://arxiv.org/abs/2401.00812'><button class="paper-btn">PAPER</button></a>
<br>
<em style="font-family: 'Times New Roman', Times, serif; font-size: smaller;">The Wizard survey explores the synergy between code and large language models (LLMs), highlighting how code empowers LLMs and benefits LLM when they serve as intelligent agents. We emphasized code‚Äôs readability, symbolic abstraction, and graph structure, presenting it as a valuable component in LLMs‚Äô training corpus.</em>
</div>
</div> -->
<!-- 
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">AAAI 23</div><img src='/images/ADEPT-framework.png' alt="yang2022adept" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
<h3>ADEPT: A DEbiasing PrompT Framework</h3>
<span style="font-family: Georgia, serif; font-size: smaller;"><b>Ke Yang</b>, Charles Yu, Yi Fung, Manling Li, Heng Ji</span>
<br>
<a href='https://arxiv.org/abs/2211.05414'><button class="paper-btn">PAPER</button></a> <a href='https://github.com/EmpathYang/ADEPT/'><button class="code-btn">CODE</button></a> <a href='files/ADEPT.pdf'><button class="slide-btn">SLIDE</button></a>
<br>
<em style="font-family: 'Times New Roman', Times, serif; font-size: smaller;">ADEPT introduces a novel debaising loss function based on counterfactual bias and manifold learning insights. "Prompt" here refers to prompt-tuning (peft) rather than prompt-engineering.</em>
</div>
</div> -->

<!-- <h2>
    <img src="/images/Zempath.png" alt="Icon" style="display: inline-block; vertical-align: middle; width: 50px;">
    <span id='jump'>Zempath</span> 
</h2> -->

<!-- In the promotional video for Zempath, we unveil our driving inspirations and fundamental principles. We showcase the seamless user experience of engaging in chats, posting either anonymously or under one's real name, indulging in conversations with our personalized chatbots, and forging new connections with like-minded individuals. Let's delve into a snippet from this captivating video:

![Zempath](/images/Zempath_display.gif) -->

<!-- Miscellaneous
------
Although I was born and raised in Shanghai, China, my true origins trace back to a serene and lesser-known village in Anhui. It's there that my family is the proud custodian of a golden paddy field and a haven for wild geese!

I am an amateur novelist, [painter](/images/hey_you.jpg), and photographer. I take photos of [cats](/images/cat.jpg), [my sister](/images/my_cool_sister.jpg), [grandparents](/images/my_grandparents.jpg), [friends](/images/on_my_21th_birthday.png), [campus](/images/campus.png), etc., in my spare time. -->