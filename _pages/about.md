---
permalink: /
title: "Welcome!"
excerpt: "Welcome!"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---
Hi there! I'm Jinglin Jian (ÁÆÄÈùñÁê≥), a recent master graduate from UIUC. I'm thrilled to have been admitted to the PhD program at [Scripps Research Institute](https://www.scripps.edu/) by the beautiful ocean üèñÔ∏è at San Diego, CA. Since 2023, I studied at the **School of Information Sciences** at UIUC, where I had the opportunity to work with Professor [Haohan Wang](https://haohanwang.github.io/). Previously, I received a bachelor‚Äôs degree in a computer-related field from Beijing Normal University, where I contributed as a research assistant in Professor [Jingjing Zhang](https://scholar.google.com/citations?user=TJxt0-0AAAAJ&hl=en)'s lab. Additionaly, out of personal interest, I also earned a second bachelor‚Äôs degree in **Economics** from **Peking University**.

<style>
table, td, th, tr {
   border: none!important;
   font-size: 14px;
}
</style>

<h2><span>Publications and Conferences</span></h2>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CS582 ML for Bioinformatics Workshop</div><img src='/images/2024geocm.png' alt="2024geocm" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
<h3>GeoCM: Exploring Consistency Models and EGNNs for Molecular 3D Structure Prediction</h3>
<span style="font-family: Georgia, serif; font-size: smaller;"> Ruibo Hou, <b>Jinglin Jian</b>, Dian Zhou</span>
<br>
<a href='https://openreview.net/pdf?id=B1Ok8gBHfh'><button class="paper-btn">Paper</button></a> 
<!-- <a href='https://github.com/EmpathYang/Prejudice-Caprice-Framework/'><button class="code-btn">CODE</button></a> -->
<br>
<em style="font-family: 'Times New Roman', Times, serif; font-size: smaller;">Developed a self-supervised model based on the Equivariant Graph Neural Networks (EGNN) and Consistency Models (CM) to predict molecular 3D structures.</em>
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">BigData 2024</div><img src='/images/2024multimodal.png' alt="2024multimodal" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
<h3>Patient Outcome Predictions via A Multimodal Lan- guage Model for Electronic Health Records</h3>
<span style="font-family: Georgia, serif; font-size: smaller;">Zihan Li, <b>Jinglin Jian</b>, Chundian Li, Jinxia Yao, Jin Chen, Yang Zhang</span>
<br>
<a href='https://drive.google.com/drive/folders/1I12WiZpvVcawr0PKcvEPsnRuffrxnSjZ'><button class="paper-btn">Paper</button></a> 
<!-- <a href='https://github.com/EmpathYang/Prejudice-Caprice-Framework/'><button class="code-btn">CODE</button></a> -->
<br>
<em style="font-family: 'Times New Roman', Times, serif; font-size: smaller;">Early prediction of mortality risk and hospital length of stay is critical. We propose a multimodal framework that integrates full-text clinical note embeddings and time-stamped physiological data to jointly model patient outcomes.</em>
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">BigData 2024</div><img src='/images/2024aptamer.png' alt="2024aptamer" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
<h3>Big Data-Driven Computational Aptamer Design Framework via Parallel Monte Carlo Tree Search</h3>
<span style="font-family: Georgia, serif; font-size: smaller;"><b>Jinglin Jian</b>, Zhiheng Jiao, Zihan Li, Jin Chen</span>
<br>
<a href='10.1109/BigData62323.2024.10825454'><button class="paper-btn">Paper</button></a> 
<!-- <a href='https://github.com/EmpathYang/Prejudice-Caprice-Framework/'><button class="code-btn">CODE</button></a> -->
<br>
<em style="font-family: 'Times New Roman', Times, serif; font-size: smaller;">Developed an enhanced parallel Monte Carlo Tree Search framework for designing aptamers with high-affinity and specificity for target proteins.</em>
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">arXiv 2024</div><img src='/images/TAIS-framework.png' alt="2024tais" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
<h3>Toward a Team of AI-made Scientists for Scientific Discovery from Gene Expression Data</h3>
<span style="font-family: Georgia, serif; font-size: smaller;">Haoyang Liu, Yijiang Li, <b>Jinglin Jian</b>, Yuxuan Cheng, Jianrong Lu, Shuyi Guo, Jinglei Zhu, Mianchen Zhang, Miantong Zhang, Haohan Wang</span>
<br>
<a href='https://arxiv.org/abs/2402.12391'><button class="paper-btn">Paper</button></a> 
<!-- <a href='https://github.com/EmpathYang/Prejudice-Caprice-Framework/'><button class="code-btn">CODE</button></a> -->
<br>
<em style="font-family: 'Times New Roman', Times, serif; font-size: smaller;">ML can discover disease-predictive genes from gene expression data. We introduced the Team of AI-made Scientists (TAIS), a LLM-based framework for automatic streamlining ML analysis. TAIS consists of simulated roles, including a project manager, data engineer, and domain expert.</em>
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">AIED 2024</div><img src='/images/2024visual-aid.jpg' alt="2024visual-aid" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
<h3>"Which Animal Would You Like to See on Your Flashcards?" Designing Visual Aids Together with Kids Using GIMs</h3>
<span style="font-family: Georgia, serif; font-size: smaller;"> Yiqi Xiao and <b>Jinglin Jian</b></span>
<br>
<a href='https://visual-aid-2165b82d929e.herokuapp.com/'><button class="paper-btn">Website</button></a> 
<br>
<em style="font-family: 'Times New Roman', Times, serif; font-size: smaller;">
Visual aids enhance children's learning. Educational theories emphasize student agency. We developed a platform based on image generative models, specifically tailored for children with autism.
</em>
</div>
</div>


<h2><span>Selected Projects</span></h2>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Research Assistant</div><img src='/images/2020hypervideo.png' alt="2020hypervideo" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
<h3>The Impact of Productive Failure on Learning Performance and Cognitive Load: Using Hypervideo to Facilitate Online Interactions</h3>
<span style="font-family: Georgia, serif; font-size: smaller;">Xiaojie Niu, Jingjing Zhang, Kate M. Xu, Xuan Wang</span>
<br>
<a href='https://ieeexplore.ieee.org/document/9499919'><button class="paper-btn">Paper</button></a> 
<br>
<em style="font-family: 'Times New Roman', Times, serif; font-size: smaller;">Productive failure is an instructional approach that uses students' cognitive conflicts to enhance their learning. This experimental study investigated the effect of productive failure in a hypervideo environment.</em>
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Bachelor‚Äôs Thesis</div><img src='/images/2021eduKG.png' alt="2021eduKG" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
<h3>Semi-automatic Knowledge Graph Construction</h3>
<span style="font-family: Georgia, serif; font-size: smaller;">Jinglin Jian (Advisor: Prof. Qinhua Zheng)</span>
<br>
<a href='https://JianJinglin.github.io/files/2021eduKG.pdf'><button class="paper-btn">Paper (in Chinese)</button></a>
<br>
<em style="font-family: 'Times New Roman', Times, serif; font-size: smaller;">An interactive system was designed and implemented, enabling domain experts to collaborate with AI models to create educational knowledge graphs (KG) from unstructured text (i.e. lecture transcripts).</em>
</div>
</div>

<!-- 
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICLR Workshop 2024</div><img src='/images/Wizard-agent.png' alt="yang2024llm" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
<h3>If LLM Is the Wizard, Then Code Is the Wand: A Survey on How Code Empowers Large Language Models to Serve as Intelligent Agents</h3>
<span style="font-family: Georgia, serif; font-size: smaller;"><b>Ke Yang</b>\*, Jiateng Liu\*, John Wu, Chaoqi Yang, Yi R. Fung, Sha Li, Zixuan Huang, Xu Cao, Xingyao Wang, Yiquan Wang, Heng Ji, Chengxiang Zhai  (* indicates equal contributions)</span>
<br>
<a href='https://arxiv.org/abs/2401.00812'><button class="paper-btn">PAPER</button></a>
<br>
<em style="font-family: 'Times New Roman', Times, serif; font-size: smaller;">The Wizard survey explores the synergy between code and large language models (LLMs), highlighting how code empowers LLMs and benefits LLM when they serve as intelligent agents. We emphasized code‚Äôs readability, symbolic abstraction, and graph structure, presenting it as a valuable component in LLMs‚Äô training corpus.</em>
</div>
</div> -->
<!-- 
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">AAAI 23</div><img src='/images/ADEPT-framework.png' alt="yang2022adept" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
<h3>ADEPT: A DEbiasing PrompT Framework</h3>
<span style="font-family: Georgia, serif; font-size: smaller;"><b>Ke Yang</b>, Charles Yu, Yi Fung, Manling Li, Heng Ji</span>
<br>
<a href='https://arxiv.org/abs/2211.05414'><button class="paper-btn">PAPER</button></a> <a href='https://github.com/EmpathYang/ADEPT/'><button class="code-btn">CODE</button></a> <a href='files/ADEPT.pdf'><button class="slide-btn">SLIDE</button></a>
<br>
<em style="font-family: 'Times New Roman', Times, serif; font-size: smaller;">ADEPT introduces a novel debaising loss function based on counterfactual bias and manifold learning insights. "Prompt" here refers to prompt-tuning (peft) rather than prompt-engineering.</em>
</div>
</div> -->

<!-- <h2>
    <img src="/images/Zempath.png" alt="Icon" style="display: inline-block; vertical-align: middle; width: 50px;">
    <span id='jump'>Zempath</span> 
</h2> -->

<!-- In the promotional video for Zempath, we unveil our driving inspirations and fundamental principles. We showcase the seamless user experience of engaging in chats, posting either anonymously or under one's real name, indulging in conversations with our personalized chatbots, and forging new connections with like-minded individuals. Let's delve into a snippet from this captivating video:

![Zempath](/images/Zempath_display.gif) -->

<!-- Miscellaneous
------
Although I was born and raised in Shanghai, China, my true origins trace back to a serene and lesser-known village in Anhui. It's there that my family is the proud custodian of a golden paddy field and a haven for wild geese!

I am an amateur novelist, [painter](/images/hey_you.jpg), and photographer. I take photos of [cats](/images/cat.jpg), [my sister](/images/my_cool_sister.jpg), [grandparents](/images/my_grandparents.jpg), [friends](/images/on_my_21th_birthday.png), [campus](/images/campus.png), etc., in my spare time. -->